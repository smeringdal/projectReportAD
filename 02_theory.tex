\chapter{Theory}
\label{ch:theory}
\section{Automatic differentiation}
\label{sec:AD}
Automatic differentiation (AD) is a method that makes the computer derive the derivatives with very little effort from the user. If you have not heard of AD before, the first thing that you might think of is algebraic or symbolic differentiation. In this type of differentiation the computer learns the basic rules from calculus like e.g.
\begin{align*}
    &\frac{d}{dx}x^n     = n\cdot x^{n-1} \\
    &\frac{d}{dx}cos(x)  = -sin(x) \\
    &\frac{d}{dx}\exp{x} = \exp{x} \\
\end{align*}
etc. and the chain- and product rule
\begin{align*}
    &\frac{d}{dx}f(x)\cdot g(x) = f'(x)\cdot g(x) + f(x)\cdot g'(x) \\
    &\frac{d}{dx}f(g(x)) = g'(x)\cdot f'(g(x)).
\end{align*}
The computer will then use these rules on symbolic variables to obtain the derivative of any function given. This will give perfectly accurate derivatives, but the disadvantage with this approach is that it is computational demanding and as f(x) becomes more complex, the calculations will become slow.

If AD is not symbolic differentiation you might think that it is finite differences, where you use the definition of the derivative
\begin{equation*}
    \frac{df}{dx} = \frac{f(x+h) - f(x)}{h}
\end{equation*}
with a small $h$ to obtain the numerical approximation of the derivative of $f$. This approach is not optimal because, first of all, if you choose an $h$ too small, you will get problems with rounding errors on your computer. This is because when h is small, you will subtract two very similar numbers, $f(x+h)$ and $f(x)$ and then divide by a small number $h$. This means that any small rounding errors in the subtraction, that may occur i.e. because of machine precision, will be hugely magnified by the division. Secondly, if you choose $h$ too large your approximation of the derivative will not be accurate. This is called the truncation error. Hence in finite differences you have the problem that you need a small step size $h$ to reduce the truncation error, but $h$ can not be too small, because the you get round-off errors. Hence the finite differences method is an unstable method and is not what we call AD.

AD can be split into two different methods - forward AD and backward AD. Both methods are similar to symbolic differentiation in the way that we implement the differentiation rules, but they differ by instead of differentiating symbols and then inserting values for the symbols, we keep track of the function values and the corresponding values of the derivatives as we go. Both methods does this by separating each expression into a finite set of elementary operations. 

\subsection{Forward Automatic Differentiation}
In forward AD, the function and derivative value is stored in a tuple $[\cdot, \cdot]$. In this way, we can continuously update both the function value and the derivative value for every operation we perform on the function. 

As an example, consider the scalar function $f = f(x)$ with its derivative $f_x$ where $x$ is a scalar variable. Then the AD-variable $x$ is the pair $[x, 1]$ and for f we have [$f$, $f_x$]. In the pair $[x,1]$, $x$ is the numerical value of $x$ and $1 = \frac{dx}{dx}$. Similar for $f(x)$ where $f$ is the numerical value of $f(x)$ and $f_x$ the numerical value of $f'(x)$.  We then define the arithmetic operators for such pairs such that for functions $f$ and $g$,
\begin{equation}
    \begin{aligned}
    &\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big] \pm \big[g \hspace{0.5em} , \hspace{0.5em}  g_x\big] = \big[f \pm g \hspace{0.5em} , \hspace{0.5em}  f_x \pm g_x\big] \hspace{0.5em}, \\\\
    &\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]\cdot \big[g \hspace{0.5em} , \hspace{0.5em}  g_x\big] = \big[f\cdot g \hspace{0.5em} , \hspace{0.5em}  f_x\cdot g + f\cdot g_x\big],\\\\
    &\frac{\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]}{\big[g \hspace{0.5em} , \hspace{0.5em}  g_x\big]} = \Bigg[\frac{f}{g} \hspace{0.5em} , \hspace{0.5em}  \frac{f_x\cdot g - f\cdot g_x}{g^2}\Bigg].
\end{aligned}
\label{eq:arithmeticOperators}
\end{equation}

It is also necessary to define the chain rule such that for a function h(x)
\begin{equation*}
h\big(f(x)\big) = h\bigg(\big[f \hspace{0.5em} , \hspace{0.5em} f_x\big]\bigg) = \big[h(f) \hspace{0.5em} , \hspace{0.5em}  f_x\cdot h'(f)\big]. 
\end{equation*}
The only thing that remains to define are the rules concerning elementary functions like
\begin{equation}
    \begin{aligned}
    &\exp\bigg(\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]\bigg) =  \big[\exp(f) \hspace{0.5em} , \hspace{0.5em}  \exp(f)\cdot f_x\big],\\
    &\log\bigg(\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]\bigg) =  \Big[\log(f) \hspace{0.5em} , \hspace{0.5em}  \frac{f_x}{f}\Big],\\
    &\sin\bigg(\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]\bigg) =  \big[\sin(f) \hspace{0.5em} , \hspace{0.5em}  \sin(f)\cdot f_x\big]\text{,  etc.}\\
\end{aligned}
\label{eq:elementaryFunctions}
\end{equation}

When these arithmetic operators and the elementary function are implemented you are able to calculate any scalar function derivative without actually doing any form of differentiation yourselves. Let us look at an step by step example where 
\begin{equation}
    \label{eq:forwardADExample}
    f(x) = x\cdot\exp(2x) \hspace{3em} \text{for}\hspace{1em} x = 2.
\end{equation}
Then the declaration of the AD-variable gives $x = [2 \hspace{0.5em} , \hspace{0.5em}  1]$. All scalars can be looked at as AD variables with derivative equal to 0 such that
\begin{align*}
    2x &= [2 \hspace{0.5em} , \hspace{0.5em}  0]\cdot [2 \hspace{0.5em} , \hspace{0.5em} 1] \\
    &=[2\cdot2 \hspace{0.5em} , \hspace{0.5em}  0\cdot1 + 2\cdot 1]\\
    &=[4 \hspace{0.5em} , \hspace{0.5em} 2].
\end{align*}
After this computation we get from the exponential
\begin{align*}
    \exp(2x) &= \exp\big([4 \hspace{0.5em} , \hspace{0.5em} 2]\big)\\
    &= [\exp(4) \hspace{0.5em} , \hspace{0.5em} \exp(4)\cdot 2],
\end{align*}
and lastly from the product rule we get the correct tuple for $f(x)$
\begin{align*}
    x\cdot \exp(2x) &= [2 \hspace{0.5em} , \hspace{0.5em}  1]\cdot [\exp(4)\hspace{0.5em}, \hspace{0.5em} 2\cdot\exp(4)]\\
    &=[2\cdot\exp(4) \hspace{0.5em} , \hspace{0.5em}  1\cdot \exp(4) + 2\cdot 2\dcot\exp(4)]\\
    [f\hspace{0.5em},\hspace{0.5em} f_x] &=[2\cdot\exp(4) \hspace{0.5em} , \hspace{0.5em} 5\cdot\exp(4)].
\end{align*}
This result is equal
\begin{equation*}
    \big(f(x) \hspace{0.5em},\hspace{0.5em} f_x(x)\big) = \big(x\cdot\exp(2x) \hspace{0.5em},\hspace{0.5em} (1+2x)\exp(2x)\big)
\end{equation*}
for $x = 2$.


\subsection{Dual Numbers}
One approach to implementing forward AD is by dual numbers. Similar to complex numbers dual numbers are defined as 
\begin{equation}
    a + b\epsilon.
    \label{eq:DualNumbers}
\end{equation}
Here $a$ and $b$ are scalars and corresponds to the function value and the derivative value. $\epsilon$ is like we have for complex numbers $i^2 = -1$, but the corresponding relation for dual numbers are $\epsilon^2 = 0$. The convenient part of implementing forward AD with dual numbers is that you get the differentiation rules for arithmetic operations for free. Consider the dual numbers $x$ and $y$ on the form of definition \eqref{eq:DualNumbers}. Then we get for addition
\begin{align*}
x+y &= (a+b\epsilon)+(c+d\epsilon)\\
    &= a+c+(b+d)\epsilon,
\end{align*}
for multiplication
\begin{align*}
x\cdot y &= (a+b\epsilon)\cdot(c+d\epsilon)\\
    &= ac + (ad + bc)\epsilon + bd\epsilon^2 \\
    &= ac + (ad + bc)\epsilon,
\end{align*}
and for division
\begin{align*}
\frac{x}{y} &= \frac{a+b\epsilon}{c+d\epsilon}\\
    &=\frac{a+b\epsilon}{c+d\epsilon} \cdot \frac{c-d\epsilon}{c-d\epsilon}\\
    &=\frac{ac-(ad-bc)\epsilon-bd\epsilon^2}{c^2-d\epsilon^2}\\
    &=\frac{a}{c} + \frac{bc-ad}{c^2}\epsilon.
\end{align*}
This is very convenient, but how does dual numbers handle elementary functions like $\sin$, $\exp$, $\log$ etc? If we look at the Taylor expansion of a function $f(x)$ where x is a dual number, we get
\begin{align*}
    f(x) = f(a+b\epsilon) &= f(a) + \frac{f'(a)}{1!}(b\epsilon) + \frac{f''(a)}{2!}(b\epsilon)^2+...\\
        &=f(a) + f'(a)b\epsilon.
\end{align*}
This means that to make dual numbers handle elementary functions, the first order Taylor expansion needs to be implemented. This equals the implementation of elementary differentiation rules described in equations \eqref{eq:elementaryFunctions}. 

The weakness of implementing AD with dual numbers is clear for functions with multiple variables. Let the function $f$ be defined as $f(x,y,z) = x\cdot y + z$. Let us say we want to know the function value for $(x,y,z) = (2,3,4)$ together with all the derivatives of $f$. First we evaluate $f$ with $x$ as the only varying parameter and the rest as constants:
\begin{align*}
    f(x,y,z) &= (2+1\epsilon)\cdot(3+0\epsilon) + (1+0\epsilon)\\
        &=7+3\epsilon.
\end{align*}
$7$ is now the function value of $f$, while $3$ is the derivative value of $f$ with respect to $x$, $f_x$. To obtain $f_y$ and $f_z$ we need two more function evaluations with respectively $y$ and $z$ as the varying parameters. This example illustrates the weakness of forward AD implemented with dual numbers - when the function evaluated have $n$ input variables, we need $n$ function evaluations to determine the gradient of the function.

\subsection{Backward Automatic Differentiation}
\label{sec:BackwardAD}
The main disadvantage with forward AD is when there are many input variables and you want the derivative with respect to all variables. This is where Backward AD is a more efficient way of obtaining the derivatives. To explain backward AD it is easier to first consider the approach for forward AD, where the method also can be be explained as an extensive use of the chain rule
\begin{equation}
    \label{eq:chainRule}
    \frac{\partial f}{\partial t} = \sum_i\left(\frac{\partial f}{\partial u_i}\cdot\frac{\partial u_i}{\partial t}\right).
\end{equation}
Take $f(x) = x\cdot\exp(2x)$ like in the forward AD example \eqref{eq:forwardADExample}. We then split up the function into a sequence of elementary functions
\begin{equation}
    \label{eq:BackwardADSeperationSimple}
    x, \hspace{2em} g_1 = 2x, \hspace{2em} g_2 = \exp(g_1), \hspace{2em} g_3 = x\cdot g_2,
\end{equation}
where clearly $f(x) = g_3$. If we want the derivative of $f$ with respect to $x$ we can obtain expressions for all $g$'s by using the chain rule \eqref{eq:chainRule}
\begin{align*}
     \frac{\partial x}{\partial x} &= 1, \\
     \frac{\partial g_1}{\partial x} &= 2, \\
     \frac{\partial g_2}{\partial x} &= \frac{\partial}{\partial g_1}\exp(g_1)\cdot\frac{\partial g_1}{\partial x} = 2\exp(2x).
\end{align*}
Lastly by calculating the derivative of $g_3$ with respect to $x$ in the same matter yields the expression for the derivative of f
\begin{align*}
    \frac{\partial f}{\partial x} &= \frac{\partial g_3}{\partial x}\\
    &=\frac{\partial x}{\partial x}\cdot g_2 + x\cdot\frac{\partial g_2}{\partial x}\\
    &= \exp(2x) + x\cdot 2\exp(2x) \\
    &= (1+2x)\exp(2x).
\end{align*}
This shows how forward AD actually uses the chain rule on a sequence of elementary functions with respect to the independent variables, in this case $x$. Backward AD also uses the chain rule, but in the opposite direction. It uses it with respect to dependent variables. The chain rule then has the form
\begin{equation}
    \label{eq:chainRuleReverse}
    \frac{\partial s}{\partial u} = \sum_i\left(\frac{\partial f_i}{\partial u}\cdot\frac{\partial s}{\partial f_i}\right),
\end{equation}
for some s to be chosen. In the same example with $f(x) = x\cdot\exp(2x)$ and with the same sequence of elementary functions like in \eqref{eq:BackwardADSeperationSimple} gives the expressions from the chain rule \eqref{eq:chainRuleReverse}
\begin{align*}
    \frac{\partial s}{\partial g_3} &= \text{Unknown}\\
    \frac{\partial s}{\partial g_2} &= \frac{\partial g_3}{\partial g_2} \cdot \frac{\partial s}{\partial g_3} &&\Longleftrightarrow \quad x\cdot \frac{\partial s}{\partial g_3} \\
    \frac{\partial s}{\partial g_1} &= \frac{\partial g_2}{\partial g_1}\cdot \frac{\partial s}{\partial g_2} &&\Longleftrightarrow \quad g_2 \cdot \frac{\partial s}{\partial g_2} \\
    \frac{\partial s}{\partial x} &= \frac{\partial g_3}{\partial x}\cdot \frac{\partial s}{\partial g_3} + \frac{\partial g_1}{\partial x}\cdot \frac{\partial s}{\partial g_1} && \Longleftrightarrow \quad g_2\cdot \frac{\partial s}{\partial g_3} + 2\cdot \frac{\partial s}{\partial g_1}.\\
\end{align*}
By substituting $s$ with $g_3$ gives
\begin{align*}
    \frac{\partial g_3}{\partial g_3} &= 1\\
    \frac{\partial g_3}{\partial g_2} &= x\\
    \frac{\partial g_3}{\partial g_1} &= \exp(2x)\cdot x\\
    \frac{\partial g_3}{\partial x} &= \exp(2x)\cdot 1 + 2\cdot\exp(2x)\cdot x = (1 + 2x)\exp(2x),\\
\end{align*}
hence we obtain the correct derivative $f_x$. By now you might wonder why make this much effort to obtain the derivative of f compared to just using forward AD. The answer to this comes by looking at a more complex function with multiple input parameters. Let $f(x,y,z) = z(\sin(x^2)+yx)$ and 
\begin{equation*}
    g_1 = x^2, \quad g_2 = x\cdot y, \quad g_3 = \sin(g_1), \quad g_4 = g_2 + g_3, \quad g_5 = z\cdot g_4.
\end{equation*}
Now the derivatives from the chain rule in equation \eqref{eq:chainRuleReverse} becomes \\
\begin{multicols}{3}
    \noindent
    \begin{align*}
        \frac{\partial s}{\partial g_5} &= \text{Unknown}\\
        \frac{\partial s}{\partial g_4} &= z\cdot\frac{\partial s}{\partial g_5}\\
        \frac{\partial s}{\partial g_3} &= \frac{\partial s}{\partial g_4}
    \end{align*}
    \begin{align*}
        \frac{\partial s}{\partial g_2} &= \frac{\partial s}{\partial g_4} \\
        \frac{\partial s}{\partial g_1} &= \cos(g_1)\frac{\partial s}{\partial g_3}\\
        \frac{\partial s}{\partial x}   &= 2x\cdot\frac{\partial s}{\partial g_1} + y\cdot\frac{\partial s}{\partial g_2}
    \end{align*}
    \begin{align*}
        \frac{\partial s}{\partial y}   &= x\cdot\frac{\partial s}{\partial g_2}\\
        \frac{\partial s}{\partial z}   &= g_4\cdot\frac{\partial s}{\partial g_5}
    \end{align*}
\end{multicols}

substituting $s$ with $g_5$ yields
\begin{multicols}{3}
    \noindent
    \begin{align*}
        \frac{\partial g_5}{\partial g_5} &= 1\\
        \frac{\partial g_5}{\partial g_4} &= z\\
        \frac{\partial g_5}{\partial g_3} &= z
    \end{align*}
    \begin{align*}
        \frac{\partial g_5}{\partial g_2} &= z\\
        \frac{\partial g_5}{\partial g_1} &= \cos(x^2)\cdot z\\
        \frac{\partial g_5}{\partial x}   &= 2x\cdot\cos(x^2)\cdot z + yz
    \end{align*}
    \begin{align*}
        \frac{\partial g_5}{\partial y}   &= xz\\
        \frac{\partial g_5}{\partial z}   &= \sin(x^2)+xy
    \end{align*}
\end{multicols}
The calculation of the derivatives together with a dependency graph can be seen in \autoref{fig:depency_graph_backward_AD}. This shows that we get all the derivatives of $f(x) = g_5$ with a single function evaluation!
\begin{figure}[htbp]
	\centering
	    \includegraphics[width=0.9\textwidth]{figures/dependency_graph_backward_AD.pdf}
	    \caption{NOT A FINISHED FIGURE. Just a quick sketch of the idea I have of visualising the process of backward AD as i felt it got a bit messy with all the expressions. Gladly receiving comments on the thought/suggestions to make it more clear}
	\label{fig:depency_graph_backward_AD}
\end{figure}
Comparing this to the method of Dual Numbers were we would have to evaluate $f$ three times, one for each derivative, this is a big improvement. This illustrates the strength of backward AD - no matter how many input parameters a function have, you only need one function evaluation to get all the derivatives of a function. The disadvantage of backward AD is that when we implement this, in differ from what we did in the example above when we did it by hand, we still only want to carry along the function- and the derivative values as we did in forward AD. This means that we have to implement the dependency tree shown in \autoref{fig:depency_graph_backward_AD}. This makes the implementation of backward AD much harder than for forward AD and a bad implementation of this tree will reduce the advantage of backward AD. Also if the function is a vector function and not a scalar function, backward AD needs to run $m$ times if $f: \Re^n \rightarrow \Re^m$, hence if $n\approx m$, forward AD and backward AD will have approximately the same complexity. This is some of the reason why we here will have focus on implementing forward AD.

\subsection{Forward Automatic Differentiation with multiple parameters}
When we are dealing with functions with many input parameters and we wish to implement a forward AD, there are more efficient ways of doing this than implementing with Dual Numbers. Knut-Andreas Lie describes in \emph{\citet{lieMrstUrl}} a method where we do not need $n$ function evaluations for $n$ input parameters. Say we have a scalar function $f: \Re^n \rightarrow \Re$ and we want to obtain the gradient of $f$. Then the main idea is when we define our AD-variables, instead of having each AD-variable storing only the derivative with respect to itself, they store their gradient to the corresponding space they are in. This means we have to define what we call our primary variables which is all the variables in the relevant space. Say we have three variables $x$, $y$ and $z$ and for any function $f(x,y,z)$ we are interested in finding the gradient of $f$, $\nabla f=(f_x,\hspace{0.5em} f_y,\hspace{0.5em} f_z)^\top$. To achieve this we define the corresponding AD-variables
\begin{equation*}
    [x\hspace{0.5em},\hspace{0.5em}(1,0,0)^\top]\hspace{2em},\hspace{2em}
    [y\hspace{0.5em},\hspace{0.5em}(0,1,0)^\top]\hspace{2em},\hspace{2em}
    [z\hspace{0.5em},\hspace{0.5em}(0,0,1)^\top].
\end{equation*}
Each primary AD-variable now not only store their derivative with respect to themselves, but the gradient. The operators defined in Equations \ref{eq:arithmeticOperators} and the elementary functions in Equations \ref{eq:elementaryFunctions} are still valid, but instead of scalar products they are now vector products. As an example let $f(x,y,z) = xyz$ and $x = 1$, $y = 2$ and $z = 3$, then
\begin{align*}
    xyz &= [1\hspace{0.5em},\hspace{0.5em}(1,0,0)^\top]\cdot [2\hspace{0.5em},\hspace{0.5em}(0,1,0)^\top]\cdot
    [3\hspace{0.5em},\hspace{0.5em}(0,0,1)^\top] \\
    &=[1\cdot 2\cdot 3\hspace{0.5em},\hspace{0.5em}2\cdot3\cdot(1,0,0)^\top + 1\cdot3\cdot(0,1,0)^\top + 1\cdot2\cdot(0,0,1)^\top] \\
    [f\hspace{0.5em},\hspace{0.5em}\nabla f] &= [6\hspace{0.5em},\hspace{0.5em}(6,3,2)^\top].
\end{align*}
This result is equal to the tuple
\begin{equation*}
    \big(f(x,y,z) \hspace{0.5em},\hspace{0.5em} \nabla f(x,y,z)\big) = \big(xyz \hspace{0.5em},\hspace{0.5em} (yz,xz,xy)^\top\big)
\end{equation*}
for the corresponding $x$, $y$ and $z$ values. In numerical applications, as we are dealing with discretizations, the functions we evaluate are usually  vector functions and not scalar functions, hence $f: \Re^n \rightarrow \Re^m$. Then it is the Jacobian of $f$ we are interested in
\begin{align*}
    \mat{J}  =
    \begin{bmatrix}
        \frac{\partial f_1}{\partial x_1} & \dotsb & \frac{\partial f_1}{x_n}\\
        \vdots & \ddots & \vdots \\
        \frac{\partial f_m}{\partial x_1} & \dotsb & \frac{\partial f_m}{x_n}
    \end{bmatrix}.
\end{align*}
The forward AD method described above will be similar for a vector function as it was for a scalar function above, but there will be two differences in particular. The first one is that the primal variables needs to be initialized with their Jacobians and not just a gradient vector. The Jacobian for a primary variable of dimension $n$ will be the $n \times n$ identity matrix. The second change is that when evaluating new functions depending on the primary variables, the Jacobians corresponding to the functions will be calculated with matrix operations instead of vector operations as seen above. 

\section{Applications of Automatic Differentiation}
\label{sec:ApplicationsAD}
AD can be used in a wide spectre of applications, but in common for them all is that we have a function, vector or scalar, we either want to minimise or find the roots of. The simplest example for finding roots is for a scalar function $f$ with a scalar input $x$. Then the iteration scheme
\begin{equation*}
    x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)},
\end{equation*}
for an initial $x_0$, will converge to a root of $f$ given that it satisfies the assumptions made in the derivation of the formulae. This is called the Newton-Raphson method \textbf{is it necessary to have reference here? If so what kind of?}. With AD this is super easy as you only have to define the function $f(x)$ and then the AD finds $f'(x)$ automatically and you can use the Newton-Raphson method directly. Exactly the same approach can be used to solve linear systems in multiple dimensions. Let us look at the linear system 
\begin{equation}
    \textbf{A}\boldsymbol{x} = \textbf{b}.
    \label{eq:linearSystem}
\end{equation}
But instead of looking at it like in equation \eqref{eq:linearSystem} we write it on residual form such that
\begin{equation*}
	\boldsymbol{F}(\boldsymbol{x}) = \textbf{A}\boldsymbol{x} - \boldsymbol{b} = 0 .
\end{equation*}
This means that to solve the linear system in equation \eqref{eq:linearSystem}, we need to find the root of $\boldsymbol{F}(\boldsymbol{x})$. This can be done by choosing an initial value $\boldsymbol{x}^0$ and observe that since $\boldsymbol{F}(\boldsymbol{x})$ is linear, this will converge in one step using the multivariate Newton-Raphson method. The general form of the multivariate Newton-Raphson method is given by
\begin{equation}
	\boldsymbol{x}^{n+1} = \boldsymbol{x}^n - \left[J_{\boldsymbol{F}}  (\boldsymbol{x}^n)\right]^{-1} \boldsymbol{F}(\boldsymbol{x}^n).
    \label{eq:newtonRaphsonVector}
\end{equation}
Here $\left[J_{\boldsymbol{F}}  (\boldsymbol{x}^n)\right]^{-1}$ is the inverse of the Jacobian of $\boldsymbol{F}$ at $\boldsymbol{x}^n$. For a simple linear system like equation \eqref{eq:linearSystem} it may seem a bit forced and not necessary to make the effort to use AD to solve for $\boldsymbol{x}$. We could just as well have used some built in linear solver instead. But for numerical application we can with this method easily solve non-linear equations that we obtain from the PDEs by looking at the residual form of the equations we obtain and use the Newton-Raphson method \eqref{eq:newtonRaphsonVector} with AD. As a preface to introducing how we will do this, consider the Poisson equation
\begin{equation}
    -\nabla(\textbf{K}\nabla u) = q,
    \label{eq:Poisson}
\end{equation}
where \textbf{K} is an diffusion coefficient and we want to find $u$ on $\Omega \in \Re^d$. Numerically this can be done by using a finite volume method. This approach is based on applying conservation laws inside the domain. By dividing the domain into smaller cells, $\Omega_i$, we can instead of looking at the Poisson equation in differential form, integrate it on each cell such that
\begin{equation}
    \int_{\partial\Omega_i} - \textbf{K} \nabla u \cdot \textbf{n} ds = \int_{\Omega_i} q dA.
    \label{eq:conservation}
\end{equation}
Here \textbf{n} is the unit normal to the cell $\Omega_i$, so equation \eqref{eq:conservation} describes the conservation of mass in the cell $\Omega_i$ where total flux in and out of the boundary of $\Omega_i$ is equal to the total production in $\Omega_i$. For simplicity we define $\textbf{v} = - \textbf{K} \nabla u$ as the flux. As a simple example to begin with, we will consider \autoref{fig:gridTwoCells} who shows two cells $\Omega_i$ and $\Omega_k$. They both have a value $u_i$ and $u_k$ in the centre of the cell and the boundary, or facet, between the cells is defined as $\Gamma_{i,k}$.
\begin{figure}[htb]
    \centering
    \includegraphics[width = 0.7\textwidth]{figures/grid_two_cells.pdf}
    \caption{}
    \label{fig:gridTwoCells}
\end{figure}

 Now the flux through the boundary $\Gamma_{i,k}$ can be computed by
\begin{equation}
    v_{i,k} = \int_{\Gamma_{i,k}} \textbf{v}\cdot\textbf{n}_{i,k} ds 
    \label{eq:fluxOverBoundary}
\end{equation}
If we let $L_{i,k}$ be the length of $\Gamma_{i,k}$, then the integral in \eqref{eq:fluxOverBoundary} can be approximated by the midpoint rule with $\tilde{\textbf{v}}_{i,k}$ as the flux on the midpoint of $\Gamma_{i,k}$ 
\begin{equation*}
    v_{i,k} \approx L_{i,k}\tilde{\textbf{v}}_{i,k} \cdot \textbf{n}_{i,k} = -L_{i,k}\textbf{K}\nabla \tilde{u}_{i,k} \cdot \textbf{n}_{i,k}.
\end{equation*}
Here $\tilde{u}_{i,k}$ is the value of $u$ at the centre of the facet $\Gamma_{i,k}$. The problem we now face is that in the finite volume method, we only have the value of $u$ at the center of cell $\Omega_i$, $u_i$, and not on the facet, $\tilde{u}_{i,k}$. This means that finding the gradient of $u$ on $\Gamma_{i,k}$ using the approximation
\begin{equation*}
    v_{i,k} \approx L_{i,k}\textbf{K}_i\frac{(\tilde{u}_{i,k} - u_i)\textbf{c}_{i,k}}{|\textbf{c}_{i,k}|^2} \cdot \textbf{n}_{i,k},
\end{equation*}
can not be computed directly. Here $\textbf{c}_{i,k}$ is the vector from $u_i$ to $\tilde{u}_{i,k}$ as seen in \autoref{fig:gridTwoCells} . To handle this we first define what we call a transmissibility matrix where
\begin{equation}
    T_{i,k}(\tilde{u}_{i,k} - u_i) = L_{i,k}\textbf{K}_i\frac{(\tilde{u}_{i,k} - u_i)\textbf{c}_{i,k}}{|\textbf{c}_{i,k}|^2} \cdot \textbf{n}_{i,k}.
    \label{eq:transmissibility}
\end{equation}
Because we know that the amount of flux from cell $\Omega_i$ to $\Omega_k$ must be the same as from $\Omega_k$ to $\Omega_i$ only with opposite sign. This gives us the relations $v_{i,k} = -v_{k,i}$ and from \autoref{fig:gridTwoCells} it is clear that $\tilde{u}_{i,k} = \tilde{u}_{k,i}$. Hence we have the relation 
\begin{equation*}
    v_{i,k} = T_{i,k}(\tilde{u}_{i,k} - u_i) \hspace{3em} -v_{i,k} = T_{k,i}(\tilde{u}_{i,k} - u_k)
\end{equation*}
By subtraction the two equations for $v_{k,i}$ and moving $T_{i,k}$ and $T_{k,i}$ to the other side 
\begin{equation}
    \begin{aligned}
        (T_{i,k}^{-1} + T_{k,i}^{-1}) v_{i,k} &= (\tilde{u}_{i,k} - u_i) - (\tilde{u}_{i,k} - u_k)
        \\
        v_{i,k} &= (T_{i,k}^{-1} + T_{k,i}^{-1})^{-1}(u_k - u_i) \\
        v_{i,k} &= T_{i,k}(u_k - u_i)
    \end{aligned}
    \label{eq:flux}
\end{equation}
we manage to eliminate $\tilde{u}_{i,k}$ and get a computable expression for the gradient of $u$. This is called the two-point flux-approximation (TPFA) \emph{\citep{lieMrstUrl}}. Now that we have an approximation of the flux through the boundary between $\Omega_i$ and $\Omega_k$ we get that equation \eqref{eq:fluxOverBoundary} can be approximated by 
\begin{equation}
    \sum_k T_{i,k}(u_k - u_i) = q_i, \hspace{3em} \forall \Omega_i \in \Omega
    \label{eq:PoissonSolvableTwoCells}
\end{equation}
Where $q_i$ is the total production in cell $\Omega_i$. Now we can get a linear system of the form \textbf{A}\textbf{u} = \textbf{b},  which on residual form becomes \textbf{F}(\textbf{u}) = \textbf{A}\textbf{u} - \textbf{b} = 0 where
\begin{align*}
    \textbf{A}_{i,j} = 
    \left\lbrace
    \begin{array}{lr}
    \sum_k T_{i,k} \hspace{3em}&\text{if } j = 1\\
    -T_{i,j} \hspace{3em}&\text{if } j \neq i
    \end{array}
    \right.
\end{align*}
This means we can solve the Poisson equation \eqref{eq:Poisson}, using the scheme explained in \eqref{eq:newtonRaphsonVector} and by having $u$ as an AD-variable. But we still end up with a linear system of equations that we may as well solve without AD. 

To show the real elegance of using AD to solve PDE's we want to create a framework where we have defined discrete divergence and gradient operators such that we can write the discrete equations we want to solve on a similar form as in the continuous case. We also want to be able to do this no matter how complex and unstructured grid we have. Instead of the simple two-cell grid we used in \autoref{fig:gridTwoCells} consider a more complex grid like we have in \autoref{fig:partialGrid}. The figure shows a part of some larger grid. To define the discrete divergence and gradient operators we need some information about the topology of the grid. The grid consist of three types of objects: cells, facets and nodes. The cells are each $\Omega_i \subset \Omega$ and in our two-dimensional case the facets are simply the lines between each cell. The nodes are defined points based on the method we use where we want to find the value of the function we search. In the case of \autoref{fig:gridTwoCells} we had two nodes, $u_i$ and $u_k$. Each cell and facet has physical properties like area or length and centroid or centre. The facets also has a normal vector. \autoref{fig:partialGrid} shows how we have two different mappings that explains the relation between the cells and the facets where the values dependent on cell 5 and 8 are written out. The first mapping $F(c)$ is mapping from cell $c$ to facet $f$. The second mapping, $C_i(f)$ for $i = 1,2$, is a mapping from a facet $f$ to the two cells $C_1$ and $C_2$ that share this facet. All these properties will be used to create the discrete divergence and gradient operators.
\begin{figure}[htb]
    \centering
    \includegraphics[width = 0.7\textwidth]{figures/grid_cells_facets.pdf}
    \caption{}
    \label{fig:partialGrid}
\end{figure}

We now have all the physical properties of the grid we used to attain the formulae in equation \eqref{eq:PoissonSolvableTwoCells}, but now we want to create discrete divergence and gradient operators that corresponds to the continuous equivalents for this grid. Consider the Poisson equation \eqref{eq:Poisson} for the function $u$. Then the gradient operator for a facet $f$ is defined as 
\begin{equation}
    \texttt{gradient}(u)[f] = u[C_2(f)] - u[C_1(f)] 
    \label{eq:discreteGradient}
\end{equation}
when $u[C_i(f)]$ is the value of u at the cell corresponding to $C_i(f)$. For the divergence operator, we remember the expression we found for the flux through a facet in equation \eqref{eq:flux}. Let $v_{i,k} = v[f]$ when f is the facet between cell $i$ and cell $k$. Since the divergence in a cell is the same as the sum of flux leaving and entering the cell, the discrete divergence operator for a cell $c$ is defined as 
\begin{equation*}
    \texttt{divergence}(\textbf{v})[c] = \sum_{f\in f(c)} \text{sgn}(f)\textbf{v}[f]
\end{equation*}
where the function sgn($f$) is defined as 
\begin{align*}
    \text{sgn}(f) = \left\lbrace
    \begin{array}{rl}
        1 \hspace{3em}&\text{if } c \in C_1(f)\\
        -1 \hspace{3em}&\text{if } c \in C_2(f).
    \end{array}
    \right.
\end{align*}
Hence we can now, only based on the topology of the grid, create discrete divergence and gradient operators such that the discrete Poisson equations we want to solve, can now be written very similar as for the continuous case:
\begin{align*}
    -\nabla(\textbf{K}\nabla u) - q &= 0 \\
    \textbf{F}(\textbf{u}) = \texttt{divergence}(\textbf{T }\texttt{gradient}(\textbf{u}))-\textbf{q} &= 0.
\end{align*}
Here \textbf{T} is the transmissibility matrix defined in \eqref{eq:transmissibility}. We can see how similar the notation for the discrete equations are to the continuous equation. We can actually read the discrete expression and directly understand what equation we are trying to solve. For this simple Poisson equation, we will still have a linear system and we would not necessarily need to use AD to solve it, but for more complex problems, we can derive the discrete divergence and gradient operators in the same approach for any type of grid and although the system becomes non-linear it will be easy to solve using AD and Newton-Raphson method. An example of this can be seen in \autoref{sec:FlowSolver}.

 




























