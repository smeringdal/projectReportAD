\chapter{Conclusion and Future Work}
\label{ch:Conclusion}
This project report consisted of discussing the difference in method and advantages between forward and backward Automatic Differentiation (AD), the applications of AD, and implementation of forward AD in Julia. The implementation has been benchmarked against other AD libraries. Finally, an example where AD is used in a real world example for solving partial differential equations (PDEs) that describe the flow inside an oil reservoir has been provided.

My implementation of forward AD in Julia (\textit{ForwardAutoDiff}) performs well in the benchmark for evaluating the value and the Jacobian of a standard vector function with three vector inputs. For vectors of lengths less than 1000, but more than 50, \autoref{fig:benchmarkLongVectors} and \autoref{fig:benchmarkAllADs} show that it is the fastest implementation tested. It is interesting to see in the benchmarking (\autoref{fig:benchmarkADInLoop}) that there is an opportunity to gain computational efficiency by using for-loops in Julia. Currently, \textit{ForwardAutoDiff} is implemented similarly as the one in MATLAB, using vectorization. MATLAB has the limitation of having a lot of overhead if you use for-loops -- since Julia does not have this limitation, it would be interesting to look further into different implementations of AD in Julia to see if it is possible to make a more efficient implementation by a more extensive use of for-loops. \autoref{fig:benchmarkAllADs} shows that the built-in AD implementation in Julia (\textit{ForwardDiff}) has very little overhead for small vectors, but scales badly as the size of the vectors increase. Even tough it scales badly for long vectors, considering the low overhead for small problems, an interesting approach would be to use this library as a building brick of a new implementation of \textit{ForwardAutoDiff}. Since Julia has proven itself to have little overhead when using for-loops, this may lead to a better implementation of \textit{ForwardAutoDiff} in Julia.

For PDEs it has been demonstrated that by introducing discrete divergence and gradient operators we can solve the PDEs describing the flow of oil in a reservoir elegantly by using a finite volume method and AD. Using these operators we can write the discrete equations on a very similar form as the continuous. By setting up the equations as a function on residual form, we can solve it by using AD and the Newton-Raphson method to find the roots of the function. For the real world example in \autoref{ch:FlowSolver}, the AD implementation in MATLAB solves the problem quicker than the implementation in Julia for all discretizations tested. The discretization with fewest cells gives a system of 1002 equations. So even tough \textit{ForwardAutoDiff} was the fastest at evaluating a vector function (\autoref{fig:benchmarkAllADs}) for this length, it is still slower at solving the example. This indicates that when the function evaluated becomes more complex, and the Jacobian less sparse and diagonal, the AD tool in MATLAB handles it better than \textit{ForwardAutoDiff}. Hence it would be interesting to see if, with new implementations, either with less vectorization or with using \textit{ForwardDiff} as a building brick, the computational efficency of the flow solver example or other real world examples can be improved by using Julia.

% This investigation would also consist of testing the implementations further for different types of functions that gives different structures to the Jacobians